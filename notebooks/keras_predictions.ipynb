{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "#from keras.layers import LSTR\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.linear_model import SGDRegressor as SGD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters and Model Architecture\n",
    "N_LAYERS = 5 #50 #25\n",
    "N_NODES = 100 #50\n",
    "TEST_SIZE = 0.20 #0.25\n",
    "DAYS_TO_CLIP = 50 #35\n",
    "EPOCHS = 4 #8\n",
    "\n",
    "#tf.random.set_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back_5</th>\n",
       "      <th>back_4</th>\n",
       "      <th>back_3</th>\n",
       "      <th>back_2</th>\n",
       "      <th>back_1</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>-0.022141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>-0.022141</td>\n",
       "      <td>-0.002490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>-0.022141</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>-0.002498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>-0.003916</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.014397</td>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.014397</td>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>-0.014397</td>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>-0.001391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>-0.008388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-29</th>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>-0.008388</td>\n",
       "      <td>0.010551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14619 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              back_5    back_4    back_3    back_2    back_1  Adj Close\n",
       "Date                                                                   \n",
       "1962-01-02       NaN       NaN       NaN       NaN       NaN   0.014743\n",
       "1962-01-03       NaN       NaN       NaN       NaN  0.014743   0.002435\n",
       "1962-01-04       NaN       NaN       NaN  0.014743  0.002435  -0.022141\n",
       "1962-01-05       NaN       NaN  0.014743  0.002435 -0.022141  -0.002490\n",
       "1962-01-08       NaN  0.014743  0.002435 -0.022141 -0.002490  -0.002498\n",
       "...              ...       ...       ...       ...       ...        ...\n",
       "2020-01-23 -0.003916 -0.003785 -0.014397 -0.005788 -0.006271  -0.006762\n",
       "2020-01-24 -0.003785 -0.014397 -0.005788 -0.006271 -0.006762  -0.024112\n",
       "2020-01-27 -0.014397 -0.005788 -0.006271 -0.006762 -0.024112  -0.001391\n",
       "2020-01-28 -0.005788 -0.006271 -0.006762 -0.024112 -0.001391  -0.008388\n",
       "2020-01-29 -0.006271 -0.006762 -0.024112 -0.001391 -0.008388   0.010551\n",
       "\n",
       "[14619 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify location of time series data\n",
    "file_path = '../data/interim/time_series.csv'\n",
    "\n",
    "def read_data(file):\n",
    "    \"\"\"\n",
    "    Read csv data from the specified file location.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file, index_col='Date')\n",
    "    return df\n",
    "\n",
    "\n",
    "# read time series data\n",
    "time_series_df = read_data(file_path)\n",
    "\n",
    "# test_df will be used to test on recent market performance\n",
    "test_df = read_data(file_path) \n",
    "\n",
    "\n",
    "def clip_recent_days(df, n_days):\n",
    "    \"\"\"\n",
    "    remove recent days from data frame\n",
    "    \"\"\"\n",
    "    return(df[:-n_days])\n",
    "\n",
    "\n",
    "# clip most recent days\n",
    "time_series_df = clip_recent_days(time_series_df, DAYS_TO_CLIP)\n",
    "\n",
    "# inspect the time series dataframe\n",
    "time_series_df\n",
    "\n",
    "#time_series_df = pd.read_csv('../data/interim/time_series.csv', index_col='Date')\n",
    "#test_df = time_series_df.copy()\n",
    "#time_series_df = time_series_df[:-DAYS_TO_CLIP]\n",
    "#time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14619 entries, 1962-01-02 to 2020-01-29\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   back_5     14614 non-null  float64\n",
      " 1   back_4     14615 non-null  float64\n",
      " 2   back_3     14616 non-null  float64\n",
      " 3   back_2     14617 non-null  float64\n",
      " 4   back_1     14618 non-null  float64\n",
      " 5   Adj Close  14619 non-null  float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 799.5+ KB\n"
     ]
    }
   ],
   "source": [
    "time_series_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              back_5    back_4    back_3    back_2    back_1\n",
       " Date                                                        \n",
       " 2020-01-29 -0.006271 -0.006762 -0.024112 -0.001391 -0.008388,\n",
       " Date\n",
       " 2020-01-29    0.010551\n",
       " Name: Adj Close, dtype: float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tomorrow(df):\n",
    "    tomorrow = df.tail(1)\n",
    "    predictors = tomorrow.iloc[:,:5]\n",
    "    target = tomorrow.iloc[:,-1]\n",
    "    return predictors, target\n",
    "    \n",
    "tomorrows_predictors, tomorrows_target = get_tomorrow(time_series_df)\n",
    "\n",
    "get_tomorrow(time_series_df)\n",
    "    \n",
    "#tomorrow = time_series_df.tail(1)\n",
    "#tomorrows_predictors = tomorrow.iloc[:,:5]\n",
    "#tomorrows_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictors_and_targets(df):\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    predictors = df[['back_5', 'back_4', 'back_3', 'back_2', 'back_1']].values\n",
    "    assert type(predictors) is np.ndarray\n",
    "    \n",
    "    n_cols = predictors.shape[1]\n",
    "    \n",
    "    targets = df[['Adj Close']].values\n",
    "    assert type(targets) is np.ndarray\n",
    "    \n",
    "    return predictors, targets, n_cols\n",
    "\n",
    "\n",
    "#time_series_df = time_series_df.dropna()\n",
    "#time_series_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of predictors\n",
    "#predictors = time_series_df[['back_5', 'back_4', 'back_3', 'back_2', 'back_1']].values\n",
    "#n_cols = predictors.shape[1]\n",
    "#type(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of targets\n",
    "#target = time_series_df[['Adj Close']].values\n",
    "#type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back_5</th>\n",
       "      <th>back_4</th>\n",
       "      <th>back_3</th>\n",
       "      <th>back_2</th>\n",
       "      <th>back_1</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14614.000000</td>\n",
       "      <td>14615.000000</td>\n",
       "      <td>14616.000000</td>\n",
       "      <td>14617.000000</td>\n",
       "      <td>14618.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.013902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.266947</td>\n",
       "      <td>-0.266947</td>\n",
       "      <td>-0.266947</td>\n",
       "      <td>-0.266947</td>\n",
       "      <td>-0.266947</td>\n",
       "      <td>-0.266947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.006677</td>\n",
       "      <td>-0.006678</td>\n",
       "      <td>-0.006679</td>\n",
       "      <td>-0.006678</td>\n",
       "      <td>-0.006681</td>\n",
       "      <td>-0.006680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.007891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.164755</td>\n",
       "      <td>0.164755</td>\n",
       "      <td>0.164755</td>\n",
       "      <td>0.164755</td>\n",
       "      <td>0.164755</td>\n",
       "      <td>0.164755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             back_5        back_4        back_3        back_2        back_1  \\\n",
       "count  14614.000000  14615.000000  14616.000000  14617.000000  14618.000000   \n",
       "mean       0.000579      0.000579      0.000577      0.000577      0.000576   \n",
       "std        0.013902      0.013902      0.013903      0.013903      0.013902   \n",
       "min       -0.266947     -0.266947     -0.266947     -0.266947     -0.266947   \n",
       "25%       -0.006677     -0.006678     -0.006679     -0.006678     -0.006681   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.007891      0.007891      0.007891      0.007891      0.007891   \n",
       "max        0.164755      0.164755      0.164755      0.164755      0.164755   \n",
       "\n",
       "          Adj Close  \n",
       "count  14619.000000  \n",
       "mean       0.000577  \n",
       "std        0.013902  \n",
       "min       -0.266947  \n",
       "25%       -0.006680  \n",
       "50%        0.000000  \n",
       "75%        0.007891  \n",
       "max        0.164755  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#all_columns = ['back_5', 'back_4', 'back_3', 'back_2', 'back_1', 'Adj Close']\n",
    "#time_series_df[all_columns] = scaler.fit_transform(time_series_df[all_columns])\n",
    "#predictors = scaler.transform(predictors)\n",
    "#targets = scaler.transform(targets)\n",
    "#min(predictors), max(predictors), min(targets), max(targets)\n",
    "#time_series_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, targets, n_cols = format_predictors_and_targets(time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "predictors = X_scaler.fit_transform(predictors)\n",
    "targets = y_scaler.fit_transform(targets)\n",
    "\n",
    "#min(predictors), max(predictors), min(targets), max(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors, targets, test_size=TEST_SIZE, shuffle=False, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65251087, 0.62400053, 0.56707184, 0.61259195, 0.61257317],\n",
       "       [0.62400053, 0.56707184, 0.61259195, 0.61257317, 0.60674955],\n",
       "       [0.56707184, 0.61259195, 0.61257317, 0.60674955, 0.64152164],\n",
       "       ...,\n",
       "       [0.58746467, 0.63725901, 0.57748002, 0.65269278, 0.60863907],\n",
       "       [0.63725901, 0.57748002, 0.65269278, 0.60863907, 0.6348846 ],\n",
       "       [0.57748002, 0.65269278, 0.60863907, 0.6348846 , 0.59761716]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65269278, 0.60863907, 0.6348846 , 0.59761716, 0.56392263],\n",
       "       [0.60863907, 0.6348846 , 0.59761716, 0.56392263, 0.59447476],\n",
       "       [0.6348846 , 0.59761716, 0.56392263, 0.59447476, 0.6932492 ],\n",
       "       ...,\n",
       "       [0.58500937, 0.60495237, 0.60383365, 0.6026948 , 0.56250502],\n",
       "       [0.60495237, 0.60383365, 0.6026948 , 0.56250502, 0.61513676],\n",
       "       [0.60383365, 0.6026948 , 0.56250502, 0.61513676, 0.59892962]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,101\n",
      "Trainable params: 41,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_sequential(n_nodes, n_layers, n_cols):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(n_nodes, activation='relu', input_shape=(n_cols,)))\n",
    "    \n",
    "    for i in range(n_layers-1):\n",
    "        model.add(Dense(n_nodes, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_sequential(N_NODES, N_LAYERS, n_cols)\n",
    "model.summary()\n",
    "        \n",
    "    \n",
    "\n",
    "#initialize a sequential keras model\n",
    "#model = Sequential()\n",
    "\n",
    "# build with N_LAYERS N_NODES each\n",
    "#model.add(Dense(N_NODES, activation='relu', input_shape=(n_cols,)))\n",
    "#for i in range(N_LAYERS-1):\n",
    "#    model.add(Dense(N_NODES, activation='relu'))\n",
    "\n",
    "\n",
    "# build final layer that will contain the output prediction\n",
    "#model.add(Dense(1))\n",
    "\n",
    "# compile the model with ADAM\n",
    "# will adjust the learning rate as it does gradient descent\n",
    "# MSE loss function is best for regression problems\n",
    "#model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# verify model structure\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "11691/11691 [==============================] - 1s 119us/step - loss: 0.0069\n",
      "Epoch 2/4\n",
      "11691/11691 [==============================] - 1s 77us/step - loss: 0.0012\n",
      "Epoch 3/4\n",
      "11691/11691 [==============================] - 1s 70us/step - loss: 0.0012\n",
      "Epoch 4/4\n",
      "11691/11691 [==============================] - 1s 68us/step - loss: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ebc8a999e50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # fit the model\n",
    "# apply back propagation and gradient descent\n",
    "model.fit(X_train, y_train, use_multiprocessing=True, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,101\n",
      "Trainable params: 41,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# save and load model\n",
    "model.save('../models/keras_dense.h5')\n",
    "model = load_model('../models/keras_dense.h5')\n",
    "\n",
    "# verify model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.57496595], dtype=float32), array([0.6798047], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "min(predictions), max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01873289], dtype=float32), array([0.02652618], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = y_scaler.inverse_transform(predictions)\n",
    "min(predictions), max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predictions\n",
    "output = pd.DataFrame(output)\n",
    "output.to_csv('../data/interim/keras_dense_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be: min < 1.0 and max > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9814414978027344, 1.0268810987472534)\n"
     ]
    }
   ],
   "source": [
    "predictions = np.exp(predictions)\n",
    "min_pred = float(min(predictions))\n",
    "max_pred = float(max(predictions))\n",
    "\n",
    "print((min_pred, max_pred))\n",
    "\n",
    "is_good_model = min_pred<=1.0 and max_pred>=1.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.15027113]), array([0.15863058]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "min(y_test), max(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.86047465]), array([1.17190494]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_exp = np.exp(y_test)\n",
    "min(y_test_exp), max(y_test_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int('stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def revert_exp(predictions, y_test):\n",
    "    predictions = np.exp(predictions)\n",
    "    y_test_exp = np.exp(y_test)\n",
    "    return predictions, y_test_exp\n",
    "\n",
    "# min(y_test_exp), max(y_test_exp)\n",
    "#########################################predictions, y_test_exp = revert_exp(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0009927 ],\n",
       "       [0.99805546],\n",
       "       [1.0035926 ],\n",
       "       ...,\n",
       "       [0.9957928 ],\n",
       "       [0.997847  ],\n",
       "       [0.99766034]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [53.964194, 45.865303, 53.878943, 53.793691]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting change in stock price with 49.743414% accuracy\n"
     ]
    }
   ],
   "source": [
    "#### this is very good\n",
    "\n",
    "def get_accuracy(y, pred):\n",
    "    \n",
    "    #scale and shift binary results\n",
    "    # -1 -> stock went down\n",
    "    # +1 -> stock increased or stayed the same\n",
    "    y = ((y>=1)*2)-1\n",
    "    pred = ((pred>=1)*2)-1\n",
    "    \n",
    "    # stocks move in the same direction when a_i*b_i is positive\n",
    "    accuracy = (np.sum((y*pred)>=0)/len(y))*100\n",
    "    \n",
    "    print(\"Predicting change in stock price with %f%s accuracy\" % (accuracy,'%'))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "scores.append(get_accuracy(y_test_exp, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_good_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Stop here'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b84a2b4da15e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stop here'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Stop here'"
     ]
    }
   ],
   "source": [
    "int('Stop here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading dates where COVID-19 affected stock market need to be excluded. if included, accuracy drops to ~48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we can make and evaluate predictions, It's time for some Cross-Validation to improve the Neural Network's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAYS_CLIPPED = [1, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "DAYS_CLIPPED = [35]\n",
    "TEST_SIZES = [0.25]\n",
    "LAYERS = [5, 10, 25]\n",
    "NODES = [25, 50, 75]\n",
    "EPOCHS = [2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DAYS_CLIPPED) * len(TEST_SIZES) * len(NODES) * len(LAYERS) * len(EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation():\n",
    "    #best_layers = 0\n",
    "    #best_nodes = 0\n",
    "    #best_test_size = 0\n",
    "    #best_days_clipped = 0\n",
    "    #best_epochs = 0\n",
    "    best_mean_accuracy = 0\n",
    "    loop = 0\n",
    "    good_models = 0\n",
    "    bad_models = 0\n",
    "    \n",
    "    # read_data\n",
    "    time_series_df = read_data(file_path)\n",
    "    \n",
    "    \n",
    "    for DAYS_TO_CLIP in DAYS_CLIPPED:\n",
    "        \n",
    "        time_series_df = clip_recent_days(time_series_df, DAYS_TO_CLIP)\n",
    "        predictors, targets, n_cols = format_predictors_and_targets(time_series_df)\n",
    "        \n",
    "        \n",
    "        for TEST_SIZE in TEST_SIZES:\n",
    "            \n",
    "            X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "            predictors = X_scaler.fit_transform(predictors)\n",
    "            targets = y_scaler.fit_transform(targets)\n",
    "            \n",
    "            #X_train, X_test, y_train, y_test = train_test_split(predictors, targets, test_size=TEST_SIZE)\n",
    "            \n",
    "            \n",
    "            for N_NODES in NODES:\n",
    "                for N_LAYERS in LAYERS:\n",
    "                    \n",
    "                    model = build_sequential(N_NODES, N_LAYERS, n_cols)\n",
    "                    \n",
    "                    \n",
    "                    for N_EPOCHS in EPOCHS:\n",
    "                        \n",
    "                        accuracies = []\n",
    "                        current_hyperparameters = {\n",
    "                                'layers': N_LAYERS,\n",
    "                                'nodes': N_NODES,\n",
    "                                'epochs': N_EPOCHS,\n",
    "                        }\n",
    "                        print('CURRENT', current_hyperparameters)\n",
    "                        \n",
    "                        loop += 1\n",
    "                        batch_max_accuracy = 0\n",
    "                        for i in range(10):\n",
    "                            \n",
    "                            print('PROGRESS: ', loop)\n",
    "                            \n",
    "                            ###\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(predictors, targets, test_size=TEST_SIZE, stratify=None, shuffle=False)\n",
    "\n",
    "                            ###\n",
    "                            \n",
    "                            model.fit(X_train, y_train, use_multiprocessing=True, epochs=N_EPOCHS)\n",
    "                            predictions = model.predict(X_test)\n",
    "                            \n",
    "                            ###\n",
    "                            predictions = y_scaler.inverse_transform(predictions)\n",
    "                            y_test = y_scaler.inverse_transform(y_test)\n",
    "                            ###\n",
    "                            \n",
    "                            predictions, y_test_exp = revert_exp(predictions, y_test)\n",
    "                            #accuracies.append(get_accuracy(y_test_exp, predictions))\n",
    "                        \n",
    "                        \n",
    "                            ### make sure model is useful\n",
    "                            min_pred = float(min(predictions))\n",
    "                            max_pred = float(max(predictions))\n",
    "                            is_good_model = min_pred<=1.0 and max_pred>=1.0\n",
    "                            good_models += is_good_model\n",
    "                            bad_models += (not is_good_model)\n",
    "                            print(is_good_model, ' Good:',good_models, ' Bad:', bad_models,' Rate:',good_models/(good_models+bad_models))\n",
    "                            ###\n",
    "                            \n",
    "                            if is_good_model:\n",
    "                                current_accuracy = get_accuracy(y_test_exp, predictions)\n",
    "                                accuracies.append(current_accuracy)\n",
    "                                \n",
    "                                if current_accuracy > batch_max_accuracy:\n",
    "                                    batch_max_accuracy = current_accuracy\n",
    "                                    batch_best_model = model\n",
    "                                    \n",
    "                            if (not is_good_model):\n",
    "                                accuracies.append(0)\n",
    "            \n",
    "                            \n",
    "\n",
    "                        \n",
    "                        mean_accuracies = 0\n",
    "                        if len(accuracies)>0:\n",
    "                            mean_accuracy = np.mean(accuracies)\n",
    "                        \n",
    "                        if mean_accuracy>best_mean_accuracy:\n",
    "                            print('NEW BEST PARAMS FOUND!')\n",
    "                            hyperparameters = {\n",
    "                                'layers': N_LAYERS,\n",
    "                                'epochs': N_EPOCHS,\n",
    "                                'nodes': N_NODES,\n",
    "                                'test_size': TEST_SIZE,\n",
    "                                'days_clipped': DAYS_TO_CLIP,\n",
    "                                'mean_accuracy': mean_accuracy,\n",
    "                                'accuracy': best_mean_accuracy\n",
    "                            }\n",
    "                            #best_layers = N_LAYERS\n",
    "                            #best_nodes = N_NODES\n",
    "                            #best_test_size = TEST_SIZE\n",
    "                            #best_days_clipped = DAYS_TO_CLIP\n",
    "                            #best_epochs = N_EPOCHS\n",
    "                            best_mean_accuracy = mean_accuracy\n",
    "                            #best_model = model\n",
    "                            best_model = batch_best_model\n",
    "                        print('BEST PARAMS:')\n",
    "                        print(hyperparameters)\n",
    "                            \n",
    "    return best_model, hyperparameters\n",
    "\n",
    "model, params = run_cross_validation()\n",
    "\n",
    "                                                      \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-bfd368200e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/best_model.h5')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-01-02</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.589844</td>\n",
       "      <td>1.578125</td>\n",
       "      <td>1.578125</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-03</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.601562</td>\n",
       "      <td>1.578125</td>\n",
       "      <td>1.601562</td>\n",
       "      <td>0.014120</td>\n",
       "      <td>1200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-04</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.613281</td>\n",
       "      <td>1.597656</td>\n",
       "      <td>1.605469</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>1088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.613281</td>\n",
       "      <td>1.566406</td>\n",
       "      <td>1.570312</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>1222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-08</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.582031</td>\n",
       "      <td>1.546875</td>\n",
       "      <td>1.566406</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>1388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>41.450001</td>\n",
       "      <td>42.180000</td>\n",
       "      <td>38.520000</td>\n",
       "      <td>39.209999</td>\n",
       "      <td>39.209999</td>\n",
       "      <td>48425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>39.880001</td>\n",
       "      <td>40.830002</td>\n",
       "      <td>39.230000</td>\n",
       "      <td>40.470001</td>\n",
       "      <td>40.470001</td>\n",
       "      <td>41301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>42.900002</td>\n",
       "      <td>43.320000</td>\n",
       "      <td>41.160000</td>\n",
       "      <td>41.240002</td>\n",
       "      <td>41.240002</td>\n",
       "      <td>48921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>41.950001</td>\n",
       "      <td>44.040001</td>\n",
       "      <td>41.470001</td>\n",
       "      <td>43.849998</td>\n",
       "      <td>43.849998</td>\n",
       "      <td>33355600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>45.459999</td>\n",
       "      <td>46.709999</td>\n",
       "      <td>41.740002</td>\n",
       "      <td>43.130001</td>\n",
       "      <td>43.130001</td>\n",
       "      <td>60208100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14669 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume\n",
       "Date                                                                       \n",
       "1962-01-02   0.000000   1.589844   1.578125   1.578125   0.013913    902400\n",
       "1962-01-03   0.000000   1.601562   1.578125   1.601562   0.014120   1200000\n",
       "1962-01-04   0.000000   1.613281   1.597656   1.605469   0.014154   1088000\n",
       "1962-01-05   0.000000   1.613281   1.566406   1.570312   0.013844   1222400\n",
       "1962-01-08   0.000000   1.582031   1.546875   1.566406   0.013810   1388800\n",
       "...               ...        ...        ...        ...        ...       ...\n",
       "2020-04-03  41.450001  42.180000  38.520000  39.209999  39.209999  48425700\n",
       "2020-04-06  39.880001  40.830002  39.230000  40.470001  40.470001  41301900\n",
       "2020-04-07  42.900002  43.320000  41.160000  41.240002  41.240002  48921800\n",
       "2020-04-08  41.950001  44.040001  41.470001  43.849998  43.849998  33355600\n",
       "2020-04-09  45.459999  46.709999  41.740002  43.130001  43.130001  60208100\n",
       "\n",
       "[14669 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('../data/raw/raw.csv', index_col=['Date'])\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>back_5</th>\n",
       "      <th>back_4</th>\n",
       "      <th>back_3</th>\n",
       "      <th>back_2</th>\n",
       "      <th>back_1</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-01-09</th>\n",
       "      <td>0.013776</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>-0.022141</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.007416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-10</th>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>-0.022141</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.007317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-11</th>\n",
       "      <td>0.013844</td>\n",
       "      <td>-0.022141</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-12</th>\n",
       "      <td>0.014051</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-15</th>\n",
       "      <td>0.014051</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.007379</td>\n",
       "      <td>0.007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>65.824982</td>\n",
       "      <td>-0.003916</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.014397</td>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>35.047399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>65.381355</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-0.014397</td>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>34.774039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-27</th>\n",
       "      <td>63.823715</td>\n",
       "      <td>-0.014397</td>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>33.600229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>63.734993</td>\n",
       "      <td>-0.005788</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>-0.008388</td>\n",
       "      <td>33.795481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-29</th>\n",
       "      <td>63.202637</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>-0.008388</td>\n",
       "      <td>0.010551</td>\n",
       "      <td>33.491372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14614 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                price    back_5    back_4    back_3    back_2    back_1  \\\n",
       "Date                                                                      \n",
       "1962-01-09   0.013776  0.014743  0.002435 -0.022141 -0.002490 -0.002498   \n",
       "1962-01-10   0.013707  0.002435 -0.022141 -0.002490 -0.002498 -0.005012   \n",
       "1962-01-11   0.013844 -0.022141 -0.002490 -0.002498 -0.005012  0.009999   \n",
       "1962-01-12   0.014051 -0.002490 -0.002498 -0.005012  0.009999  0.014815   \n",
       "1962-01-15   0.014051 -0.002498 -0.005012  0.009999  0.014815  0.000000   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "2020-01-23  65.824982 -0.003916 -0.003785 -0.014397 -0.005788 -0.006271   \n",
       "2020-01-24  65.381355 -0.003785 -0.014397 -0.005788 -0.006271 -0.006762   \n",
       "2020-01-27  63.823715 -0.014397 -0.005788 -0.006271 -0.006762 -0.024112   \n",
       "2020-01-28  63.734993 -0.005788 -0.006271 -0.006762 -0.024112 -0.001391   \n",
       "2020-01-29  63.202637 -0.006271 -0.006762 -0.024112 -0.001391 -0.008388   \n",
       "\n",
       "            Adj Close  predicted_price  \n",
       "Date                                    \n",
       "1962-01-09  -0.005012         0.007416  \n",
       "1962-01-10   0.009999         0.007317  \n",
       "1962-01-11   0.014815         0.007400  \n",
       "1962-01-12   0.000000         0.007635  \n",
       "1962-01-15  -0.007379         0.007632  \n",
       "...               ...              ...  \n",
       "2020-01-23  -0.006762        35.047399  \n",
       "2020-01-24  -0.024112        34.774039  \n",
       "2020-01-27  -0.001391        33.600229  \n",
       "2020-01-28  -0.008388        33.795481  \n",
       "2020-01-29   0.010551        33.491372  \n",
       "\n",
       "[14614 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz_df = raw_data[['Adj Close']]\n",
    "viz_df.columns = ['price']\n",
    "\n",
    "\n",
    "viz_df = pd.merge(viz_df, time_series_df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "viz_df = viz_df.dropna()\n",
    "\n",
    "X_viz = viz_df[['back_5', 'back_4', 'back_3', 'back_2', 'back_1']].values\n",
    "\n",
    "preds_viz = model.predict(X_scaler.transform(X_viz))\n",
    "preds_viz = np.exp(preds_viz)\n",
    "preds_viz = y_scaler.inverse_transform(preds_viz)\n",
    "viz_df['predicted_price'] = preds_viz\n",
    "viz_df['predicted_price'] = viz_df['price'] * viz_df['predicted_price']\n",
    "viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43.13000106811523, 56.83122108892757, True)\n"
     ]
    }
   ],
   "source": [
    "def predict_tomorrow():\n",
    "    X = tomorrows_predictors.values\n",
    "    prediction = model.predict(X)\n",
    "    prediction = np.exp(prediction)\n",
    "    adj_close = raw_data[['Adj Close']]\n",
    "    prices = adj_close.tail(1)\n",
    "    todays_price = float(prices.values[0])\n",
    "    tomorrows_price = float(todays_price) * float(prediction)\n",
    "    is_increase = bool(prediction>=1)\n",
    "    return todays_price, tomorrows_price, is_increase\n",
    "\n",
    "print(predict_tomorrow())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>back_5</th>\n",
       "      <th>back_4</th>\n",
       "      <th>back_3</th>\n",
       "      <th>back_2</th>\n",
       "      <th>back_1</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>70.223679</td>\n",
       "      <td>0.014571</td>\n",
       "      <td>0.016962</td>\n",
       "      <td>-0.038706</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.010343</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>37.679719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02</th>\n",
       "      <td>70.086800</td>\n",
       "      <td>0.016962</td>\n",
       "      <td>-0.038706</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.010343</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>37.393556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-03</th>\n",
       "      <td>69.849518</td>\n",
       "      <td>-0.038706</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.010343</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>36.798889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>70.178055</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.010343</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>37.726688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>70.944611</td>\n",
       "      <td>-0.010343</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>38.292039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>71.264038</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>38.685230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>72.943199</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>39.997158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>74.576729</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>41.339575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>74.927017</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>41.481237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-14</th>\n",
       "      <td>75.434044</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>41.645890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>75.397163</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>41.609387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>75.609192</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>41.424182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-17</th>\n",
       "      <td>75.480125</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.007109</td>\n",
       "      <td>40.920760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-18</th>\n",
       "      <td>74.945465</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.007109</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>40.439506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-21</th>\n",
       "      <td>75.848869</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.007109</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>41.043209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22</th>\n",
       "      <td>75.258888</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.007109</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>40.587407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23</th>\n",
       "      <td>75.729027</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.007109</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>-0.023151</td>\n",
       "      <td>40.890945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-24</th>\n",
       "      <td>73.995972</td>\n",
       "      <td>-0.007109</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>-0.023151</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>39.535009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-25</th>\n",
       "      <td>72.557899</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>-0.023151</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>38.636569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-29</th>\n",
       "      <td>72.290565</td>\n",
       "      <td>-0.007809</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>-0.023151</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>38.269259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>75.129829</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>-0.023151</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>40.641060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>74.890144</td>\n",
       "      <td>-0.023151</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>40.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>75.434044</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>-0.010565</td>\n",
       "      <td>40.882949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>74.641251</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>-0.010565</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>40.630495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-05</th>\n",
       "      <td>74.373917</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>-0.010565</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>40.668395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-06</th>\n",
       "      <td>75.646049</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>-0.010565</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>40.970396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-07</th>\n",
       "      <td>76.401978</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>-0.010565</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>41.620208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-08</th>\n",
       "      <td>77.065697</td>\n",
       "      <td>-0.010565</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>41.886842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-11</th>\n",
       "      <td>76.604767</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>41.689173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>75.977928</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>41.331542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>75.139053</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>40.428093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-14</th>\n",
       "      <td>75.489349</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>-0.015134</td>\n",
       "      <td>40.598790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-15</th>\n",
       "      <td>74.355492</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>-0.015134</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>39.540812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-18</th>\n",
       "      <td>74.502975</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>-0.015134</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>39.756322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-19</th>\n",
       "      <td>74.392365</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>-0.015134</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>39.796986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-20</th>\n",
       "      <td>74.161896</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>-0.015134</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>-0.009492</td>\n",
       "      <td>39.838743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-21</th>\n",
       "      <td>73.461311</td>\n",
       "      <td>-0.015134</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>-0.009492</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>39.152720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-22</th>\n",
       "      <td>75.019203</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>-0.009492</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>40.689213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>73.507393</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>-0.009492</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>39.428232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>74.337051</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>-0.009492</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>40.092765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>75.323418</td>\n",
       "      <td>-0.009492</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>40.908350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28</th>\n",
       "      <td>75.563095</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.009229</td>\n",
       "      <td>41.300981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-29</th>\n",
       "      <td>76.263695</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.009229</td>\n",
       "      <td>-0.011794</td>\n",
       "      <td>41.347291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                price    back_5    back_4    back_3    back_2    back_1  \\\n",
       "Date                                                                      \n",
       "2018-05-01  70.223679  0.014571  0.016962 -0.038706 -0.000514 -0.010343   \n",
       "2018-05-02  70.086800  0.016962 -0.038706 -0.000514 -0.010343 -0.001951   \n",
       "2018-05-03  69.849518 -0.038706 -0.000514 -0.010343 -0.001951 -0.003391   \n",
       "2018-05-04  70.178055 -0.000514 -0.010343 -0.001951 -0.003391  0.004692   \n",
       "2018-05-07  70.944611 -0.010343 -0.001951 -0.003391  0.004692  0.010864   \n",
       "2018-05-08  71.264038 -0.001951 -0.003391  0.004692  0.010864  0.004492   \n",
       "2018-05-09  72.943199 -0.003391  0.004692  0.010864  0.004492  0.023289   \n",
       "2018-05-10  74.576729  0.004692  0.010864  0.004492  0.023289  0.022147   \n",
       "2018-05-11  74.927017  0.010864  0.004492  0.023289  0.022147  0.004686   \n",
       "2018-05-14  75.434044  0.004492  0.023289  0.022147  0.004686  0.006744   \n",
       "2018-05-15  75.397163  0.023289  0.022147  0.004686  0.006744 -0.000489   \n",
       "2018-05-16  75.609192  0.022147  0.004686  0.006744 -0.000489  0.002808   \n",
       "2018-05-17  75.480125  0.004686  0.006744 -0.000489  0.002808 -0.001708   \n",
       "2018-05-18  74.945465  0.006744 -0.000489  0.002808 -0.001708 -0.007109   \n",
       "2018-05-21  75.848869 -0.000489  0.002808 -0.001708 -0.007109  0.011982   \n",
       "2018-05-22  75.258888  0.002808 -0.001708 -0.007109  0.011982 -0.007809   \n",
       "2018-05-23  75.729027 -0.001708 -0.007109  0.011982 -0.007809  0.006228   \n",
       "2018-05-24  73.995972 -0.007109  0.011982 -0.007809  0.006228 -0.023151   \n",
       "2018-05-25  72.557899  0.011982 -0.007809  0.006228 -0.023151 -0.019626   \n",
       "2018-05-29  72.290565 -0.007809  0.006228 -0.023151 -0.019626 -0.003691   \n",
       "2018-05-30  75.129829  0.006228 -0.023151 -0.019626 -0.003691  0.038524   \n",
       "2018-05-31  74.890144 -0.023151 -0.019626 -0.003691  0.038524 -0.003195   \n",
       "2018-06-01  75.434044 -0.019626 -0.003691  0.038524 -0.003195  0.007236   \n",
       "2018-06-04  74.641251 -0.003691  0.038524 -0.003195  0.007236 -0.010565   \n",
       "2018-06-05  74.373917  0.038524 -0.003195  0.007236 -0.010565 -0.003588   \n",
       "2018-06-06  75.646049 -0.003195  0.007236 -0.010565 -0.003588  0.016960   \n",
       "2018-06-07  76.401978  0.007236 -0.010565 -0.003588  0.016960  0.009943   \n",
       "2018-06-08  77.065697 -0.010565 -0.003588  0.016960  0.009943  0.008650   \n",
       "2018-06-11  76.604767 -0.003588  0.016960  0.009943  0.008650 -0.005999   \n",
       "2018-06-12  75.977928  0.016960  0.009943  0.008650 -0.005999 -0.008216   \n",
       "2018-06-13  75.139053  0.009943  0.008650 -0.005999 -0.008216 -0.011102   \n",
       "2018-06-14  75.489349  0.008650 -0.005999 -0.008216 -0.011102  0.004651   \n",
       "2018-06-15  74.355492 -0.005999 -0.008216 -0.011102  0.004651 -0.015134   \n",
       "2018-06-18  74.502975 -0.008216 -0.011102  0.004651 -0.015134  0.001982   \n",
       "2018-06-19  74.392365 -0.011102  0.004651 -0.015134  0.001982 -0.001486   \n",
       "2018-06-20  74.161896  0.004651 -0.015134  0.001982 -0.001486 -0.003103   \n",
       "2018-06-21  73.461311 -0.015134  0.001982 -0.001486 -0.003103 -0.009492   \n",
       "2018-06-22  75.019203  0.001982 -0.001486 -0.003103 -0.009492  0.020985   \n",
       "2018-06-25  73.507393 -0.001486 -0.003103 -0.009492  0.020985 -0.020358   \n",
       "2018-06-26  74.337051 -0.003103 -0.009492  0.020985 -0.020358  0.011224   \n",
       "2018-06-27  75.323418 -0.009492  0.020985 -0.020358  0.011224  0.013182   \n",
       "2018-06-28  75.563095  0.020985 -0.020358  0.011224  0.013182  0.003177   \n",
       "2018-06-29  76.263695 -0.020358  0.011224  0.013182  0.003177  0.009229   \n",
       "\n",
       "            Adj Close  predicted_price  \n",
       "Date                                    \n",
       "2018-05-01  -0.001951        37.679719  \n",
       "2018-05-02  -0.003391        37.393556  \n",
       "2018-05-03   0.004692        36.798889  \n",
       "2018-05-04   0.010864        37.726688  \n",
       "2018-05-07   0.004492        38.292039  \n",
       "2018-05-08   0.023289        38.685230  \n",
       "2018-05-09   0.022147        39.997158  \n",
       "2018-05-10   0.004686        41.339575  \n",
       "2018-05-11   0.006744        41.481237  \n",
       "2018-05-14  -0.000489        41.645890  \n",
       "2018-05-15   0.002808        41.609387  \n",
       "2018-05-16  -0.001708        41.424182  \n",
       "2018-05-17  -0.007109        40.920760  \n",
       "2018-05-18   0.011982        40.439506  \n",
       "2018-05-21  -0.007809        41.043209  \n",
       "2018-05-22   0.006228        40.587407  \n",
       "2018-05-23  -0.023151        40.890945  \n",
       "2018-05-24  -0.019626        39.535009  \n",
       "2018-05-25  -0.003691        38.636569  \n",
       "2018-05-29   0.038524        38.269259  \n",
       "2018-05-30  -0.003195        40.641060  \n",
       "2018-05-31   0.007236        40.181900  \n",
       "2018-06-01  -0.010565        40.882949  \n",
       "2018-06-04  -0.003588        40.630495  \n",
       "2018-06-05   0.016960        40.668395  \n",
       "2018-06-06   0.009943        40.970396  \n",
       "2018-06-07   0.008650        41.620208  \n",
       "2018-06-08  -0.005999        41.886842  \n",
       "2018-06-11  -0.008216        41.689173  \n",
       "2018-06-12  -0.011102        41.331542  \n",
       "2018-06-13   0.004651        40.428093  \n",
       "2018-06-14  -0.015134        40.598790  \n",
       "2018-06-15   0.001982        39.540812  \n",
       "2018-06-18  -0.001486        39.756322  \n",
       "2018-06-19  -0.003103        39.796986  \n",
       "2018-06-20  -0.009492        39.838743  \n",
       "2018-06-21   0.020985        39.152720  \n",
       "2018-06-22  -0.020358        40.689213  \n",
       "2018-06-25   0.011224        39.428232  \n",
       "2018-06-26   0.013182        40.092765  \n",
       "2018-06-27   0.003177        40.908350  \n",
       "2018-06-28   0.009229        41.300981  \n",
       "2018-06-29  -0.011794        41.347291  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month = viz_df.loc['2018-05-01':'2018-06-30']\n",
    "month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This next plot will be very nice becuase it will show the model's ability to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.figure(figsize=(15,15))\n",
    "_ = plt.plot(month['price'],  marker='.', alpha=0.9)\n",
    "_ = plt.plot(month['predicted_price'], marker='.', alpha=0.75)\n",
    "_ = plt.legend(['Actual Price', 'Predictions'])\n",
    "_ = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.tail(40).head(30)\n",
    "X_test = test_df[['back_5', 'back_4', 'back_3', 'back_2', 'back_1']].values\n",
    "test_preds = model.predict(X_test)\n",
    "test_preds = np.exp(test_preds)\n",
    "test_df['price'] = raw_data['Adj Close']\n",
    "test_df['predictions'] = test_preds\n",
    "test_df['pred'] = (test_df['predictions'] * test_df['price'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(15,15))\n",
    "_ = plt.plot(test_df['price'], marker='.')\n",
    "_ = plt.plot(test_df['pred'], marker='.')\n",
    "_ = plt.xticks(rotation=90)\n",
    "_ = plt.legend(['Actual Price', 'Predictions'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('stop notebook execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_data(predictions):\n",
    "    \n",
    "    # undo scale\n",
    "    #scaler = joblib.load('../models/MinMaxScaler.save')\n",
    "    #unscaled_deltas = scaler.inverse_transform(predictions)\n",
    "    \n",
    "    # undo log\n",
    "    #inv_log = np.exp(unscaled_deltas)\n",
    "    inv_log = np.exp(predictions)\n",
    "    \n",
    "    return inv_log\n",
    "x = revert_data(predictions)\n",
    "(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = joblib.load('../models/MinMaxScaler.save')\n",
    "unscaled_deltas = scaler.inverse_transform(predictions)\n",
    "unscaled_deltas\n",
    "\n",
    "scaler.inverse_transform([[0.81, 0.12, 3.3], [0,1, -2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[ 0.12417709, -0.08672323,  0.88525218],[-0.12340155,  0.1822511 , -0.73470684]])\n",
    "scaler.transform(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_log = np.exp(predictions)\n",
    "inv_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_log.min(), inv_log.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(14,7))\n",
    "_ = plt.plot(predictions, linestyle='None', marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Dense_model(input_shape):\n",
    "    #initialize a sequential keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    # build three layers with 100 nodes each\n",
    "    model.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "\n",
    "    # build final layer that will contain the output prediction\n",
    "    model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LSTR_model(input_shape):\n",
    "    # Matt and Druv's architecture\n",
    "    #initialize a sequential keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    # build three layers with 100 nodes each\n",
    "    model.add(LSTR(50, return_sequence=True, input_shape=input_shape))\n",
    "    model.add(LSTR(50, return_sequence=True)\n",
    "    model.add(LSTR(50))\n",
    "\n",
    "    # build final layer that will contain the output prediction\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    mode.complile(loss='mse', optimizer='adam')\n",
    "    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_to_test = [0.00001, 0.01, 1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model(input_shape)\n",
    "    \n",
    "    # use Stochastic Gradient Descent\n",
    "    optimizer = SGD(lr=lr)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = sum((predictions - y_test)**2)\n",
    "    \n",
    "    print(lr, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
